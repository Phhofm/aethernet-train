name = "2xaether_tiny_qat"
model_type = "image"
scale = 2
use_amp = false
bfloat16 = false
fast_matmul = true

[datasets.train]
type = "paired"
dataroot_gt = '/home/phips/Documents/dataset/PDM/OSISRD/v3/hr'
dataroot_lq = '/home/phips/Documents/dataset/PDM/OSISRD/v3/x2'
patch_size = 64
batch_size = 8

[datasets.val]
name = "val"
type = "paired"
dataroot_gt = '/home/phips/Documents/dataset/PDM/OSISRD/v3/validation/hr'
dataroot_lq = '/home/phips/Documents/dataset/PDM/OSISRD/v3/validation/x2'

[val]
val_freq = 5000
[val.metrics.psnr]
type = "calculate_psnr"
[val.metrics.ssim]
type = "calculate_ssim"
[val.metrics.dists]
type = "calculate_dists"
better = "lower"
[val.metrics.topiq]
type = "calculate_topiq"

[path]
pretrain_network_g = 'path/to/your/pretrained/model.pth' # we are using the fp32 pretrain for qat training

[network_g]
type = "aether_tiny"
#type = "aether_small"
#type = "aether_medium"
#type = "aether_large"

[train]
enable_qat = true # simulates integer quantization
grad_clip = true # qat can amplify small numerical errors - grad clip helps as a safety net

[train.optim_g]
type = "adamw"
lr = 2e-5 #lower learning rate for qat fine-tuning from fp32 pretrain, 1/10th of the final FP32 learning rate
betas = [ 0.9, 0.99 ]
weight_decay = 0

[train.pixel_opt]
type = "L1Loss"
loss_weight = 1.0
reduction = "mean"

[logger]
total_iter = 1000000
save_checkpoint_freq = 5000
use_tb_logger = true
