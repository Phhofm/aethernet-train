name = "2xaether_tiny_qat_finetune"
model_type = "image"
scale = 2
use_amp = false
bfloat16 = false
fast_matmul = true

[datasets.train]
type = "paired"
dataroot_gt = '/home/phips/Documents/dataset/PDM/OSISRD/v3/hr'
dataroot_lq = '/home/phips/Documents/dataset/PDM/OSISRD/v3/x2'
patch_size = 64
batch_size = 8

[datasets.val]
name = "val"
type = "paired"
dataroot_gt = '/home/phips/Documents/dataset/PDM/OSISRD/v3/validation/hr'
dataroot_lq = '/home/phips/Documents/dataset/PDM/OSISRD/v3/validation/x2'

[val]
val_freq = 5000
[val.metrics.psnr]
type = "calculate_psnr"
[val.metrics.ssim]
type = "calculate_ssim"
[val.metrics.dists]
type = "calculate_dists"
better = "lower"
[val.metrics.topiq]
type = "calculate_topiq"

[path]
pretrain_network_g = '/home/phips/Documents/GitHub/aethernet-train/neosr/experiments/2xaether_tiny/models/net_g_200000.pth' # we are using the fp32 non-qat pretrain for qat training
strict_load_g = false # The previous non-qat fp32 model will have missing keys, unexpected keys and unexpected buffers. This is fine though.

[network_g]
type = "aether_tiny"
#type = "aether_small"
#type = "aether_medium"
#type = "aether_large"

[train]
enable_qat = true # simulates integer quantization for better int8 robustness, no PTQ with calibration needed when converting, the model learned to handle reduced precision already. Will automatically disable ema (conflict). Neosr automatically prioritizes loading the params-ema weights from the pretrain if they exist, so use the non-qat pretrain i provided.
grad_clip = true # qat can amplify small numerical errors because of integer quantization - grad clip helps as a safety net

[train.optim_g]
type = "adamw"
lr = 2e-5 #lower learning rate for qat fine-tuning from fp32 pretrain, 1/10th of the final FP32 learning rate
betas = [ 0.9, 0.99 ]
weight_decay = 0

[train.pixel_opt]
type = "L1Loss"
loss_weight = 1.0
reduction = "mean"

[logger]
total_iter = 1000000
save_checkpoint_freq = 5000
use_tb_logger = true
