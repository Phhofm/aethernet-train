name = "2xaether_tiny_qat_finetune"
model_type = "image"
scale = 2
use_amp = false
bfloat16 = false
fast_matmul = true

[datasets.train]
type = "paired"
dataroot_gt = '/home/phips/Documents/dataset/PDM/OSISRD/v3/hr'
dataroot_lq = '/home/phips/Documents/dataset/PDM/OSISRD/v3/x2'
patch_size = 64
batch_size = 8

[datasets.val]
name = "val"
type = "paired"
dataroot_gt = '/home/phips/Documents/dataset/PDM/OSISRD/v3/validation/hr'
dataroot_lq = '/home/phips/Documents/dataset/PDM/OSISRD/v3/validation/x2'

[val]
val_freq = 5000
[val.metrics.psnr]
type = "calculate_psnr"
[val.metrics.ssim]
type = "calculate_ssim"
[val.metrics.dists]
type = "calculate_dists"
better = "lower"
[val.metrics.topiq]
type = "calculate_topiq"

[path]
pretrain_network_g = '/home/phips/Documents/GitHub/aethernet-train/neosr/experiments/2xaether_tiny/models/net_g_210000.pth' # we are using the fp32 non-qat pretrain for qat training
strict_load_g = false # The previous non-qat fp32 model will have missing keys, unexpected keys and unexpected buffers. This is fine though.

[network_g]
type = "aether_tiny"
#type = "aether_small"
#type = "aether_medium"
#type = "aether_large"

[train]
enable_qat = true # simulates integer quantization for better int8 robustness, no PTQ with calibration needed when converting, the model learned to handle reduced precision already. Will automatically disable ema (conflict). Neosr automatically prioritizes loading the params-ema weights from the pretrain if they exist, so use the non-qat pretrain i provided.
grad_clip = true # qat can amplify small numerical errors because of integer quantization - grad clip helps as a safety net

[train.optim_g]
type = "adamw"
lr = 5e-6 # adaptation, not exploration. We already found optimal state, we want smallest possible adjustments (survive the "stress" of 8-bit quantization) without losing the pretrain quality. We are no longer looking for steep validation metrics climb, we are looking for stability and preservation of quality. A flat, high-psnr graph is success.Therefore way lower learning rate, it should start with similiar val metrics like the pretrain we gave it. 
betas = [ 0.9, 0.99 ]
weight_decay = 0

[train.pixel_opt]
type = "L1Loss"
loss_weight = 1.0
reduction = "mean"

[train.scheduler]
type = "MultiStepLR"
milestones = [ 400000, 600000, 800000, 900000 ]
gamma = 0.5

[logger]
total_iter = 1000000
save_checkpoint_freq = 5000
use_tb_logger = true
